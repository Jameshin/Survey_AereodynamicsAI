# íŠ¹ì • í´ë” ë…¼ë¬¸íŒŒì¼ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ ìë™ìœ¼ë¡œ ë¶„ë¥˜ ë° ë¶„ì„
import openai
import json
import fitz  # PyMuPDF
import os
import re
import shutil  # íŒŒì¼ ë³µì‚¬ìš©

# OpenAI API í‚¤ ì„¤ì • (ë³¸ì¸ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)  # ìµœì‹  ë°©ì‹

# ğŸ“‚ ë…¼ë¬¸ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ (ì—¬ê¸° ìˆ˜ì •)
PAPER_FOLDER_PATH = "D:\\2025\\Readables\\Total_test"  
OUTPUT_FOLDER = "Classified_Papers"  # ì •ë¦¬ëœ ë…¼ë¬¸ì´ ì €ì¥ë  í´ë”
RESULT_JSON_PATH = "analysis_results.json"  # JSON ì €ì¥ ê²½ë¡œ
RESULT_TXT_PATH = "analysis_results.txt"  # TXT ì €ì¥ ê²½ë¡œ

# ğŸ“Œ ë…¼ë¬¸ ë¶„ë¥˜ ê¸°ì¤€ (10ì¢…)
classification_criteria = [
    "1) Data-Driven Turbulence Modeling : Improvement of turbulence models using machine learning, RANS, LES, DNS, Subgrid modeling, Closure model learning",
    "2) Shock & Boundary Layer Interaction : Shock-boundary layer interaction, SBLI, Shock detection, Supersonic flow, Shock control",
    "3) Hypersonic Flow & High-Speed Aerodynamics : Hypersonic flow, High-temperature gas dynamics, Fluid-structure interaction, FSI, Scramjet, Hypersonic vehicle",
    "4) Reduced-Order Modeling : CFD acceleration, Low-dimensional models, Aerodynamic analysis, Data-driven surrogate models, Surrogate modeling",
    "5) Aerodynamic Shape Optimization : Shape optimization, Automated fluid dynamics design, Genetic algorithm, Reinforcement learning-based optimization",
    "6) Compressible Flow Physics : Compressible flow, Supersonic, Unsteady flow, Aerodynamic performance prediction of vehicles",
    "7) Machine Learning for Flow Control : Flow control, Drag reduction, Efficiency improvement, Jet, Vortex, Flap",
    "8) Multi-Fidelity Modeling & Uncertainty Quantification : High-fidelity vs. low-fidelity, Physical constraints, Uncertainty quantification, Reliability assessment",
    "9) Scientific Machine Learning : Physics-Informed Neural Networks, PINNs, Fluid dynamics theory, Fusion of physical models",
    "10) Experimental Data Fusion & Surrogate Modeling : Wind tunnel experimental data, AI integration, Fusion of CFD and experimental data, Generative AI, Data augmentation",
    "11) Review Papers : Overall trends with a specific view point",
    "12) Etc : None of the aboves"
]

#  PDFì—ì„œ Abstract ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_abstract_from_pdf(paper_file_path):
    """PDF íŒŒì¼ì—ì„œ Abstract(ì´ˆë¡) ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
    doc = fitz.open(paper_file_path)
    text = ""

    for page in doc:
        text += page.get_text("text") + "\n"

    # Abstract ì°¾ê¸° (ë„ì–´ì“°ê¸° í¬í•¨)
    abstract_match = re.search(r"(A\s*B\s*S\s*T\s*R\s*A\s*C\s*T|ABSTRACT|Abstract)\s*([\s\S]+?)(?=\n(Introduction|INTRODUCTION|Background|BACKGROUND|Nomenclature|1\.)|$)", text)

    if abstract_match:
        return abstract_match.group(2).strip()
    else:
        return "Abstract not found"

#  PDFì—ì„œ ì´ˆê¸° 300ë‹¨ì–´ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_first_300_words(paper_file_path):
    """PDF íŒŒì¼ì—ì„œ ì²˜ìŒ 300ë‹¨ì–´ ì¶”ì¶œ"""
    doc = fitz.open(paper_file_path)
    text = ""

    # PDFì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for page in doc:
        text += page.get_text("text") + "\n"

    #print(text)

    # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    words = re.findall(r'\S+', text)  # ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìë“¤(ë‹¨ì–´) ì¶”ì¶œ

    # 500ë‹¨ì–´ê¹Œì§€ë§Œ ê°€ì ¸ì™€ì„œ ë°˜í™˜
    first_300_words = " ".join(words[:500])  # 500ë‹¨ì–´ì¼ ë•Œ ê²°ê³¼ê°€ ê´œì°®ì•˜ìŒ

    return first_300_words if first_300_words else "No text found"

#  ChatGPTì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
def generate_prompt(extract_text):
    return f"""
    ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ í•œê¸€ë¡œ ì œê³µí•´ì¤˜:

    1. **ì´ ë…¼ë¬¸ì€ ì•„ë˜ 60ì—¬ì¢… ë¶„ë¥˜ ì¤‘ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•´ì¤˜(ë³µìˆ˜ ê°€ëŠ¥). ë¶„ë¥˜ë²ˆí˜¸ë¥¼ 2ê°œ ìˆ˜ì¤€ìœ¼ë¡œ ë¶€ì—¬í–ˆì–´. ì˜ˆë¥¼ ë“¤ì–´, 1-1)ë¶€í„° 1-6)ê¹Œì§€ëŠ” ìƒìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ì¸ "1) Compressible flow physics"ì˜ í•˜ìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ë“¤ì´ì•¼.**  
       **ë¶„ë¥˜ë²ˆí˜¸ëŠ” í•˜ìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ë§Œ ê°€ì§€ê²Œ ë¶„ë¥˜í•´ì¤˜. ì˜ˆì™¸ì ìœ¼ë¡œ, ë¶„ë¥˜ë²ˆí˜¸ 13)ì€ í•˜ìœ„ ìˆ˜ì¤€ì´ ì—†ìœ¼ë‹ˆ ê·¸ëƒ¥ ìƒìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ë¡œ ì¨ì¤˜. ì¦‰, ìƒìœ„ ìˆ˜ì¤€ì€ í•˜ìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ë¥¼ ë³´ë©´ ì•„ë‹ˆê¹Œ ìƒìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ëŠ” "13) Review or survey papers"ì„ ì œì™¸í•˜ê³  ê¼­ ë¹¼ì•¼ ëœë‹¤.**
       **ê·¸ë¦¬ê³ , ë¶„ë¥˜í•  ë•Œ ë˜ë„ë¡ Title, Abstract, Keywordsë¼ëŠ” ë‹¨ì–´ì— ê°€ê¹Œìš´ ë‹¨ì–´ë“¤ì´ í•´ë‹¹ ë…¼ë¬¸ì˜ ì„±ê²©ì„ ê°€ì¥ ë§ì´ ê·œì •í•˜ê¸° ë•Œë¬¸ì—, ê·¸ ë‹¨ì–´ë“¤ì„ ë” ì¤‘ì ì ìœ¼ë¡œ ê°ì•ˆí•´ì„œ ë¶„ë¥˜í•´ì¤˜. **
       **Introductionì´ë¼ëŠ” ë‹¨ì–´ ì´í›„ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ì „ë°˜ì ì¸ ë™í–¥ê³¼ ê´€ë ¨ë˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ ë…¼ë¬¸ê³¼ ê´€ë ¨ì—†ëŠ” í‚¤ì›Œë“¤ì´ ë§ì´ ë“±ì¥í•˜ë‹ˆ ë¬´ì‹œí•˜ëŠ”ê²Œ ë‚˜ì•„.**
       **ê·¸ë˜ì„œ, ë…¼ë¬¸ì˜ íŒŒì¼ëª…(í˜¹ì€ title)ê³¼ Abstract, Keywordsì˜ ë‹¨ì–´ë“¤ê³¼ ë‚´ìš©ë“¤ë¡œ íŒë‹¨ì„ í•´ì£¼ë©´ ì¢‹ê² ì–´." **
       {", ".join(classification_criteria)}
    2. **ì´ ë…¼ë¬¸ì—ì„œì˜ ìƒˆë¡œìš´ ë°œê²¬ì´ë‚˜ ì„±ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?**  

       **ê°™ì€ ë…¼ë¬¸ì€ í•­ìƒ ë™ì¼í•œ ë¶„ë¥˜ë¥¼ ìœ ì§€í•´ì•¼ í•˜ë‹ˆ ì—¬ëŸ¬ ë²ˆ ë¶„ë¥˜í•´ë³´ê³  ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë¶„ë¥˜ë²ˆí˜¸ë“¤ì„ ì ì–´ì¤˜. ë„ˆì˜ íŒë‹¨ì´ ë§¤ë²ˆ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë§ì•˜ì–´.**
       **ê° ë…¼ë¬¸ë³„ë¡œ ë¶„ë¥˜ê°€ ëœ ë‹¤ìŒ ìƒìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ê°€ ì—¬ì „íˆ ë“¤ì–´ê°€ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, 13)ì„ ì œì™¸í•˜ê³  ìƒìœ„ ìˆ˜ì¤€ ë¶„ë¥˜ë²ˆí˜¸ë“¤ì´ ì•ˆ ë‚˜ì˜¤ê²Œ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ì¤˜.**
       **ê²°ê³¼ë¥¼ analysis_results.txtì— ì“¸ ë•Œ ì´ëª¨í‹°ì½˜ì€ ì•ˆë“¤ì–´ê°€ê²Œ í•´ì£¼ë©´ ì¢‹ê² ì–´.**

    ë…¼ë¬¸ì˜ ë‚´ìš©:
    {extract_text}

    ê²°ê³¼ í˜•ì‹ ì˜ˆì‹œ:
    1. ë¶„ë¥˜ë²ˆí˜¸: 1-2), 7-3), 11-1)
    2. ë°œê²¬ í˜¹ì€ ì„±ê³¼: 
       - (...) 
       - (...)
    """

#  ChatGPT API ìš”ì²­ í•¨ìˆ˜
def ask_chatgpt(prompt):
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "system", "content": "You are PaperBot, an AI assistant for academic paper analysis."},
                  {"role": "user", "content": prompt}],
        temperature=0
    )
    return response.choices[0].message.content
    
# ChatGPT ì‘ë‹µì—ì„œ ë¶„ë¥˜ë²ˆí˜¸ ì¶”ì¶œ
def extract_categories_from_result(result_text):
    match = re.search(r"ë¶„ë¥˜ë²ˆí˜¸:\s*([\d, )]+)", result_text)
    if match:
        categories = re.findall(r"\d+", match.group(1))  
        return categories
    return []

#  í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ì„ ë¶„ì„ ë° ë¶„ë¥˜
def extract_year_from_filename(filename):
    """íŒŒì¼ ì´ë¦„ì—ì„œ (YYYY) í˜•ì‹ì˜ ì—°ë„ë¥¼ ì¶”ì¶œ"""
    match = re.match(r"\(\s*([^\)]+)\)", filename)  # ê´„í˜¸ ì•ˆ ë‚´ìš© ì¶”ì¶œ
    if match:
        content = match.group(1)
        digits = re.findall(r"\d{4}", content)       # 4ìë¦¬ ìˆ«ìë§Œ ì°¾ê¸°
        if digits:
            return digits[0]                         # ì²« ë²ˆì§¸ ì—°ë„ë§Œ ì‚¬ìš©
    return "Unknown"

def extract_classification_and_analysis(result_text):
    """ChatGPT ì‘ë‹µì—ì„œ ë¶„ë¥˜ë²ˆí˜¸ë“¤ê³¼ ë¶„ì„ ë‚´ìš© ì¶”ì¶œ"""


    # 1. ë³µí•© ë¶„ë¥˜ë²ˆí˜¸ (ì˜ˆ: 1-2), 3-2), 13)) ëª¨ë‘ ì¸ì‹
    classification_matches = re.findall(r"\b(\d{1,2}(?:-\d+)?\))", result_text)
    classification = classification_matches if classification_matches else ["Not found"]

    # 2. ë¶„ì„ ë‚´ìš© ì¶”ì¶œ
    analysis_match = re.search(r"ë°œê²¬ í˜¹ì€ ì„±ê³¼:\s*([\s\S]+?)(?:\n\d{1,2}(?:-\d+)?\)|\Z)", result_text)
    analysis = analysis_match.group(1).strip() if analysis_match else "Not found"

    return classification, analysis

def clean_title_from_filename(filename):
    """íŒŒì¼ëª…ì—ì„œ í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ì œëª© ì¶”ì •"""
    name = filename.replace(".pdf", "")
    # ê´„í˜¸ ì•ˆ ìˆ«ì ì œê±° (ì—°ë„)
    name = re.sub(r"\(\d{4}\)", "", name)
    # í•œê¸€ ì œê±°
    name = re.sub(r"[ê°€-í£]", "", name)
    # íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°
    name = re.sub(r"[^\w\s\-]", "", name)
    return name.strip()

def get_citation_count(title):
    url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": title,
        "fields": "title,citationCount",
        "limit": 1
    }
    headers = {"User-Agent": "PaperBot"}
    try:
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()
        results = response.json()
        if results.get("data"):
            return results["data"][0].get("citationCount", 0)
    except Exception as e:
        print(f"âŒ ì¸ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return 0

def analyze_all_papers():
    results_json = []
    results_txt = []

    print(f"ğŸ“‚ í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤: {PAPER_FOLDER_PATH}\n")

    for filename in os.listdir(PAPER_FOLDER_PATH):
        if filename.endswith(".pdf"):
            file_path = os.path.join(PAPER_FOLDER_PATH, filename)
            print(f"\nğŸ“„ ë¶„ì„ ì¤‘: {filename}")

            extract_text = extract_first_300_words(file_path)
            prompt = generate_prompt(extract_text)
            result = ask_chatgpt(prompt)

            print("\nğŸ“Œ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼:")
            print(result)

            # ë¶„ë¥˜ë²ˆí˜¸ì™€ ë¶„ì„ ë‚´ìš© ì¶”ì¶œ
            classification, analysis = extract_classification_and_analysis(result)

            # íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ
            year = extract_year_from_filename(filename)

            # ì¸ìš©íšŸìˆ˜ ì¶”ì¶œ
            #clean_title = clean_title_from_filename(filename)
            #citation_count = get_citation_count(clean_title)
            
            results_json.append({
                "filename": filename,
                "year": year,
                "classification": classification,
                "analysis": analysis,
                #"citations": citation_count    
            })

            # TXT ê²°ê³¼ ì €ì¥
            results_txt.append(f"\nğŸ“„ ë…¼ë¬¸: {filename} ({year})\në¶„ë¥˜: {classification}\n{analysis}\n{'-'*80}\n")

    # JSON íŒŒì¼ ì €ì¥
    with open(RESULT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(results_json, f, ensure_ascii=False, indent=4)

    # TXT íŒŒì¼ ì €ì¥
    with open(RESULT_TXT_PATH, "w", encoding="utf-8") as f:
        f.writelines(results_txt)

    print("\nâœ… ëª¨ë“  ë…¼ë¬¸ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“œ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {RESULT_JSON_PATH}")
    print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ TXT ì €ì¥: {RESULT_TXT_PATH}")

    print("\nâœ… ëª¨ë“  ë…¼ë¬¸ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“œ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {RESULT_JSON_PATH}")
    print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ TXT ì €ì¥: {RESULT_TXT_PATH}")

# ì‹¤í–‰ (í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ ë¶„ì„)
if __name__ == "__main__":
    analyze_all_papers()

# íŠ¹ì • í´ë” ë…¼ë¬¸íŒŒì¼ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ ìë™ìœ¼ë¡œ ë¶„ë¥˜ ë° ë¶„ì„
import openai
import json
import fitz  # PyMuPDF
import os
import re
import shutil  # íŒŒì¼ ë³µì‚¬ìš©

# OpenAI API í‚¤ ì„¤ì • (ë³¸ì¸ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)  # ìµœì‹  ë°©ì‹

# ğŸ“‚ ë…¼ë¬¸ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ (ì—¬ê¸° ìˆ˜ì •)
PAPER_FOLDER_PATH = "D:\\2025\\Readables\\Total_test"  
OUTPUT_FOLDER = "Classified_Papers"  # ì •ë¦¬ëœ ë…¼ë¬¸ì´ ì €ì¥ë  í´ë”
RESULT_JSON_PATH = "analysis_results.json"  # JSON ì €ì¥ ê²½ë¡œ
RESULT_TXT_PATH = "analysis_results.txt"  # TXT ì €ì¥ ê²½ë¡œ

# ğŸ“Œ ë…¼ë¬¸ ë¶„ë¥˜ ê¸°ì¤€ (10ì¢…)
classification_criteria = [
    "1) Data-Driven Turbulence Modeling : Improvement of turbulence models using machine learning, RANS, LES, DNS, Subgrid modeling, Closure model learning",
    "2) Shock & Boundary Layer Interaction : Shock-boundary layer interaction, SBLI, Shock detection, Supersonic flow, Shock control",
    "3) Hypersonic Flow & High-Speed Aerodynamics : Hypersonic flow, High-temperature gas dynamics, Fluid-structure interaction, FSI, Scramjet, Hypersonic vehicle",
    "4) Reduced-Order Modeling : CFD acceleration, Low-dimensional models, Aerodynamic analysis, Data-driven surrogate models, Surrogate modeling",
    "5) Aerodynamic Shape Optimization : Shape optimization, Automated fluid dynamics design, Genetic algorithm, Reinforcement learning-based optimization",
    "6) Compressible Flow Physics : Compressible flow, Supersonic, Unsteady flow, Aerodynamic performance prediction of vehicles",
    "7) Machine Learning for Flow Control : Flow control, Drag reduction, Efficiency improvement, Jet, Vortex, Flap",
    "8) Multi-Fidelity Modeling & Uncertainty Quantification : High-fidelity vs. low-fidelity, Physical constraints, Uncertainty quantification, Reliability assessment",
    "9) Scientific Machine Learning : Physics-Informed Neural Networks, PINNs, Fluid dynamics theory, Fusion of physical models",
    "10) Experimental Data Fusion & Surrogate Modeling : Wind tunnel experimental data, AI integration, Fusion of CFD and experimental data, Generative AI, Data augmentation",
    "11) Review Papers : Overall trends with a specific view point",
    "12) Etc : None of the aboves"
]

# ğŸ“„ PDFì—ì„œ Abstract ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_abstract_from_pdf(paper_file_path):
    """PDF íŒŒì¼ì—ì„œ Abstract(ì´ˆë¡) ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
    doc = fitz.open(paper_file_path)
    text = ""

    for page in doc:
        text += page.get_text("text") + "\n"

    # Abstract ì°¾ê¸° (ë„ì–´ì“°ê¸° í¬í•¨)
    abstract_match = re.search(r"(A\s*B\s*S\s*T\s*R\s*A\s*C\s*T|ABSTRACT|Abstract)\s*([\s\S]+?)(?=\n(Introduction|INTRODUCTION|Background|BACKGROUND|Nomenclature|1\.)|$)", text)

    if abstract_match:
        return abstract_match.group(2).strip()
    else:
        return "Abstract not found"

# ğŸ“„ PDFì—ì„œ ì´ˆê¸° 300ë‹¨ì–´ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_first_300_words(paper_file_path):
    """PDF íŒŒì¼ì—ì„œ ì²˜ìŒ 300ë‹¨ì–´ ì¶”ì¶œ"""
    doc = fitz.open(paper_file_path)
    text = ""

    # PDFì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for page in doc:
        text += page.get_text("text") + "\n"

    #print(text)

    # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    words = re.findall(r'\S+', text)  # ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìë“¤(ë‹¨ì–´) ì¶”ì¶œ

    # 300ë‹¨ì–´ê¹Œì§€ë§Œ ê°€ì ¸ì™€ì„œ ë°˜í™˜
    first_300_words = " ".join(words[:300])

    return first_300_words if first_300_words else "No text found"

# ğŸ“ ChatGPTì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
def generate_prompt(extract_text):
    return f"""
    ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ í•œê¸€ë¡œ ì œê³µí•´ì¤˜:

    1. **ì´ ë…¼ë¬¸ì€ ì•„ë˜ 12ì¢… ë¶„ë¥˜ë²• ì¤‘ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€? (ë³µìˆ˜ ê°€ëŠ¥)**  
       {", ".join(classification_criteria)}
    2. **ì´ ë…¼ë¬¸ì—ì„œì˜ ìƒˆë¡œìš´ ë°œê²¬ì´ë‚˜ ì„±ê³¼ëŠ” ë¬´ì—‡ì¸ê°€?**  

    ë…¼ë¬¸ì˜ ë‚´ìš©:
    {extract_text}

    ê²°ê³¼ í˜•ì‹ ì˜ˆì‹œ:
    1. ë¶„ë¥˜ë²ˆí˜¸: 1), 3)
    2. ë°œê²¬ í˜¹ì€ ì„±ê³¼: 
       - (...) 
       - (...)
    """

# ğŸ“¡ ChatGPT API ìš”ì²­ í•¨ìˆ˜
def ask_chatgpt(prompt):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "You are PaperBot, an AI assistant for academic paper analysis."},
                  {"role": "user", "content": prompt}],
        temperature=0.5
    )
    return response.choices[0].message.content

# ì•ˆì „í•œ í´ë”ëª… ìƒì„± í•¨ìˆ˜ (í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def sanitize_folder_name(folder_name):
    """í´ë”ëª…ì—ì„œ í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    folder_name = re.sub(r'[ê°€-í£]', '', folder_name)  # í•œê¸€ ì œê±°
    folder_name = re.sub(r'[<>:"/\\|?*]', '', folder_name)  # Windowsì—ì„œ ë¬¸ì œë˜ëŠ” ë¬¸ì ì œê±°
    folder_name = folder_name.strip()  # ì•ë’¤ ê³µë°± ì œê±°
    return folder_name[:100]  # ë„ˆë¬´ ê¸¸ë©´ 100ìë¡œ ì œí•œ

# ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± í•¨ìˆ˜ (í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def sanitize_filename(filename):
    """íŒŒì¼ëª…ì—ì„œ í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ"""
    filename = re.sub(r'[ê°€-í£]', '', filename)  # í•œê¸€ ì œê±°
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Windows ë¬¸ì œ ë¬¸ì ì œê±°
    filename = filename.strip()  # ì•ë’¤ ê³µë°± ì œê±°
    return filename[:200]  # ìµœëŒ€ ê¸¸ì´ ì œí•œ

def copy_to_classification_folders(paper_file_path, categories):
    """ë…¼ë¬¸ì„ í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë¶„ë¥˜ í´ë”ë¡œ ë³µì‚¬"""
    base_filename = sanitize_filename(os.path.basename(paper_file_path))  # ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½

    for category in categories:
        raw_folder_name = classification_criteria[int(category)-1].split(') ')[1].split(':')[0]
        folder_name = sanitize_folder_name(f"{category}) {raw_folder_name}")  # í´ë”ëª… ì •ë¦¬
        category_folder = os.path.join(OUTPUT_FOLDER, folder_name)
        os.makedirs(category_folder, exist_ok=True)  # í´ë” ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ìƒëµ)

        dest_file_path = os.path.join(category_folder, base_filename)  # ìƒˆë¡œìš´ íŒŒì¼ ê²½ë¡œ
        shutil.copy(paper_file_path, dest_file_path)
        print(f"ğŸ“‚ {paper_file_path} â†’ {category_folder}ì— ë³µì‚¬ ì™„ë£Œ!")

# ğŸ§ ChatGPT ì‘ë‹µì—ì„œ ë¶„ë¥˜ë²ˆí˜¸ ì¶”ì¶œ
def extract_categories_from_result(result_text):
    match = re.search(r"ë¶„ë¥˜ë²ˆí˜¸:\s*([\d, )]+)", result_text)
    if match:
        categories = re.findall(r"\d+", match.group(1))  
        return categories
    return []

# ğŸš€ í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ì„ ë¶„ì„ ë° ë¶„ë¥˜
def analyze_all_papers():
    results_json = []
    results_txt = []

    print(f"ğŸ“‚ í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤: {PAPER_FOLDER_PATH}\n")

    for filename in os.listdir(PAPER_FOLDER_PATH):
        if filename.endswith(".pdf"):
            file_path = os.path.join(PAPER_FOLDER_PATH, filename)
            print(f"\nğŸ“„ ë¶„ì„ ì¤‘: {filename}")

            #extract_text = extract_abstract_from_pdf(file_path)
            extract_text = extract_first_300_words(file_path)
            prompt = generate_prompt(extract_text)
            result = ask_chatgpt(prompt)

            print("\nğŸ“Œ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼:")
            print(result)

            # JSON ê²°ê³¼ ì €ì¥
            results_json.append({"filename": filename, "analysis": result})

            # TXT ê²°ê³¼ ì €ì¥
            results_txt.append(f"\nğŸ“„ ë…¼ë¬¸: {filename}\n{result}\n{'-'*80}\n")

            # ğŸ“‚ ë…¼ë¬¸ ë¶„ë¥˜ í´ë”ë¡œ ë³µì‚¬
            categories = extract_categories_from_result(result)
            if categories:
                copy_to_classification_folders(file_path, categories)
            else:
                print("âš  ë¶„ë¥˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë³µì‚¬í•˜ì§€ ì•ŠìŒ.")

    # JSON íŒŒì¼ ì €ì¥
    with open(RESULT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(results_json, f, ensure_ascii=False, indent=4)

    # TXT íŒŒì¼ ì €ì¥
    with open(RESULT_TXT_PATH, "w", encoding="utf-8") as f:
        f.writelines(results_txt)

    print("\nâœ… ëª¨ë“  ë…¼ë¬¸ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“œ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {RESULT_JSON_PATH}")
    print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ TXT ì €ì¥: {RESULT_TXT_PATH}")

# ì‹¤í–‰ (í´ë” ë‚´ ëª¨ë“  ë…¼ë¬¸ ë¶„ì„)
if __name__ == "__main__":
    analyze_all_papers()